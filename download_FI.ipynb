{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec8d4d-c474-4612-9871-b4af9d9b90fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas numpy requests\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install selenium webdriver-manager beautifulsoup4\n",
    "#!pip install --upgrade typing_extensions\n",
    "#!pip install --upgrade selenium\n",
    "#!pip install --upgrade bleach tinycss2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecd9c3-24ef-4333-93ec-52bf90d633b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Google Drive API dependencies\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Base URLs\n",
    "csv_base_url = \"https://stooq.pl/q/d/l/?s={}.n&i=d\"\n",
    "title_base_url = \"https://stooq.pl/q/g/?s={}.n\"\n",
    "\n",
    "# Filepath for CSV\n",
    "csv_filename = \"/tmp/data.csv\"\n",
    "\n",
    "# List to store all results\n",
    "all_results = []\n",
    "\n",
    "# Function to download the CSV file\n",
    "def download_csv(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to download {url} (Status Code: {response.status_code})\")\n",
    "            return False\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "# Function to get the webpage title from the <meta> tag content using Selenium in headless mode\n",
    "\n",
    "\n",
    "def get_webpage_title(url):\n",
    "    # Set up Chrome options for headless mode (no GUI)\n",
    "    options = Options()\n",
    "    options.headless = True  # Run the browser in headless mode\n",
    "    \n",
    "    # Automatically download and manage ChromeDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    # Open the webpage URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the cookie acceptance pop-up to appear\n",
    "    try:\n",
    "        # Wait until the button is present and clickable\n",
    "        accept_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"fc-cta-consent\"))\n",
    "        )\n",
    "        accept_button.click()  # Click the accept button\n",
    "        print(\"Cookie pop-up accepted.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"No cookie acceptance pop-up found or timeout occurred:\", e)\n",
    "    \n",
    "    # Wait for the page to load fully\n",
    "    time.sleep(3)  \n",
    "    \n",
    "    # Get the rendered HTML of the page\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Look for the <title> tag\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "    else:\n",
    "        title = \"No <title> tag found\"\n",
    "    \n",
    "    # Close the Selenium driver\n",
    "    driver.quit()\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate daily returns\n",
    "def calculate_daily_returns(prices):\n",
    "    return prices.pct_change().dropna()\n",
    "\n",
    "# Function to calculate 22-day returns\n",
    "def calculate_22_day_returns(prices):\n",
    "    return prices.pct_change(periods=22).dropna()\n",
    "\n",
    "# Function to calculate statistics on 22-day returns\n",
    "def calculate_22_day_statistics(returns_22):\n",
    "    last_66_returns = returns_22.iloc[-66:]  # Use iloc for integer-based indexing\n",
    "    min_return = last_66_returns.min()\n",
    "    max_return = last_66_returns.max()\n",
    "    first_quartile = np.percentile(last_66_returns, 25)\n",
    "    return min_return, max_return, first_quartile\n",
    "\n",
    "\n",
    "# Main function to process the data\n",
    "def process_data(url, url_title, filename):\n",
    "    # Download the CSV file\n",
    "    download_csv(url, filename)\n",
    "\n",
    "    # Read the CSV into a DataFrame with proper settings\n",
    "    try:\n",
    "        df = pd.read_csv(filename, on_bad_lines='skip', delimiter=',', decimal='.', encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return\n",
    "    if df.empty:\n",
    "        print(\"The DataFrame is empty.\")\n",
    "        return \n",
    "    else:\n",
    "        # Print the first few rows to inspect the structure\n",
    "        print(\"First few rows of the CSV file:\")\n",
    "        print(df.head())\n",
    "\n",
    "        # Strip any leading or trailing whitespace from the column names\n",
    "        df.columns = df.columns.str.strip()\n",
    "\n",
    "        # Ensure the date column is in datetime format\n",
    "        date_column = 'Data'  # The actual name of the date column\n",
    "        \n",
    "        if date_column not in df.columns:\n",
    "            print(f\"Warning: Column '{date_column}' not found. Available columns: {df.columns}\")\n",
    "        return\n",
    "\n",
    "        df['date'] = pd.to_datetime(df[date_column])\n",
    "\n",
    "        # Sort the data by date in ascending order (this ensures the data is ordered)\n",
    "        df = df.sort_values(by='date')\n",
    "\n",
    "        # Calculate daily returns from the second column (prices)\n",
    "        prices_column = df.columns[1]  # Assuming the second column contains the prices\n",
    "        daily_returns = calculate_daily_returns(df[prices_column])\n",
    "\n",
    "        # Calculate 22-day returns\n",
    "        returns_22 = calculate_22_day_returns(df[prices_column])\n",
    "\n",
    "        # Calculate statistics for the last 66 22-day returns\n",
    "        min_return, max_return, first_quartile = calculate_22_day_statistics(returns_22)\n",
    "\n",
    "        # Get the newest 1-day and 22-day returns (from the most recent data point)\n",
    "        newest_1_day_return = daily_returns.iloc[-1]\n",
    "        newest_22_day_return = returns_22.iloc[-1]\n",
    "    \n",
    "        # Get the title of the webpage\n",
    "        nazwa = get_webpage_title(url_title)\n",
    "    \n",
    "        # Save the required data into an array\n",
    "        result = [\n",
    "            nazwa,  # Name of the file\n",
    "            min_return,  # Minimum of the last 66 22-day returns\n",
    "            max_return,  # Maximum of the last 66 22-day returns\n",
    "            first_quartile,  # First quartile of the last 66 22-day returns\n",
    "            newest_1_day_return,  # Newest 1-day return\n",
    "            newest_22_day_return  # Newest 22-day return\n",
    "        ]\n",
    "    \n",
    "        return result\n",
    "\n",
    "# Loop through each XXXX value\n",
    "for xxxx in range(1000, 1010):\n",
    "    csv_url = csv_base_url.format(xxxx)\n",
    "    title_url = title_base_url.format(xxxx)\n",
    "\n",
    "    print(f\"Processing: {csv_url}\")\n",
    "    result = process_data(csv_url, title_url, csv_filename)\n",
    "\n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "\n",
    "    # Delete CSV file after processing\n",
    "    if os.path.exists(csv_filename):\n",
    "        os.remove(csv_filename)\n",
    "\n",
    "    time.sleep(2)  # Optional delay\n",
    "\n",
    "# Convert results to DataFrame and save\n",
    "if all_results:\n",
    "    final_df = pd.DataFrame(all_results, columns=[\"Title\", \"Min 22-day Return\", \"Max 22-day Return\", \"1st Quartile\", \"Latest 1-day Return\", \"Latest 22-day Return\"])\n",
    "    csv_data = final_df.to_csv(index=False, header=False, sep=\";\").encode('utf-8')\n",
    "\n",
    "creds = service_account.Credentials.from_service_account_file('/tmp/credentials.json', scopes=['https://www.googleapis.com/auth/drive'])\n",
    "\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# File details\n",
    "file_name = \"oceny.csv\"\n",
    "\n",
    "# Correctly get the folder ID (replace 'Dane' with the actual folder name)\n",
    "folder_name = 'Dane'\n",
    "query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "results = drive_service.files().list(q=query, spaces='drive', fields='files(id)').execute()\n",
    "items = results.get('files', [])\n",
    "\n",
    "if items:\n",
    "    folder_id = items[0]['id']\n",
    "else:\n",
    "    print(f\"‚ùå Folder '{folder_name}' not found in Google Drive.\")\n",
    "    exit()\n",
    "\n",
    "# Now use the correct folder ID in the file search query\n",
    "query = f\"name='{file_name}' and '{folder_id}' in parents\"\n",
    "results = drive_service.files().list(q=query, spaces='drive', fields='files(id)').execute()\n",
    "items = results.get('files', [])\n",
    "\n",
    "if items:\n",
    "    file_id = items[0]['id']\n",
    "    # Update the existing file as a new version\n",
    "    # The MediaIoBaseUpload class needs to be called directly, not as an attribute of drive_service.\n",
    "    media = MediaIoBaseUpload(io.BytesIO(csv_data), mimetype='text/csv', resumable=True)\n",
    "    updated_file = drive_service.files().update(fileId=file_id, media_body=media).execute()\n",
    "    print(f\"File '{file_name}' updated as a new version. File ID: {file_id}\")\n",
    "else:\n",
    "    # If the file doesn't exist, create it\n",
    "    file_metadata = {\n",
    "        'name': file_name,\n",
    "        'parents': [folder_id] \n",
    "    }\n",
    "    # The MediaIoBaseUpload class needs to be called directly, not as an attribute of drive_service.\n",
    "    media = MediaIoBaseUpload(io.BytesIO(csv_data), mimetype='text/csv', resumable=True)\n",
    "    created_file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    file_id = created_file.get('id')\n",
    "    print(f\"File '{file_name}' created. File ID: {file_id}\")\n",
    "\n",
    "print(f\"Analysis complete. Extracted data from {len(images)} pages saved in 'oceny.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c1c21-698a-4f49-a053-b14188ef2149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
