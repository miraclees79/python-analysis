{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec8d4d-c474-4612-9871-b4af9d9b90fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install pandas numpy requests\n",
    "#!pip install beautifulsoup4\n",
    "#!pip install selenium webdriver-manager beautifulsoup4\n",
    "#!pip install --upgrade typing_extensions\n",
    "#!pip install --upgrade selenium\n",
    "#!pip install --upgrade bleach tinycss2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6daa0f-ac65-4415-9eae-6127427e4233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import io\n",
    "\n",
    "# Google Drive API dependencies\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.http import MediaIoBaseUpload\n",
    "\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# Get the temporary directory\n",
    "tmp_dir = tempfile.gettempdir()\n",
    "print(f\"Temporary directory: {tmp_dir}\")\n",
    "\n",
    "# Create a temporary file inside the temp directory # Filepath for CSV\n",
    "csv_filename = os.path.join(tmp_dir, \"data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# Base URLs\n",
    "csv_base_url = \"https://stooq.pl/q/d/l/?s={}.n&i=d\"\n",
    "title_base_url = \"https://stooq.pl/q/g/?s={}.n\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List to store all results\n",
    "all_results = []\n",
    "\n",
    "# Function to download the CSV file\n",
    "def download_csv(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to download {url} (Status Code: {response.status_code})\")\n",
    "            return False\n",
    "        with open(filename, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "# Function to get the webpage title from the <meta> tag content using Selenium in headless mode\n",
    "\n",
    "\n",
    "def get_webpage_title(url):\n",
    "    # Set up Chrome options for headless mode (no GUI)\n",
    "    options = Options()\n",
    "    options.add_argument('--headless=new')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--user-data-dir=/tmp/chrome_test_data') #added this line.\n",
    "    \n",
    "    # Automatically download and manage ChromeDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    # Open the webpage URL\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Wait for the cookie acceptance pop-up to appear\n",
    "    try:\n",
    "        # Wait until the button is present and clickable\n",
    "        accept_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, \"fc-cta-consent\"))\n",
    "        )\n",
    "        accept_button.click()  # Click the accept button\n",
    "        print(\"Cookie pop-up accepted.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"No cookie acceptance pop-up found or timeout occurred:\", e)\n",
    "    \n",
    "    # Wait for the page to load fully\n",
    "    time.sleep(3)  \n",
    "    \n",
    "    # Get the rendered HTML of the page\n",
    "    page_source = driver.page_source\n",
    "    \n",
    "    # Parse the HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "    # Look for the <title> tag\n",
    "    title_tag = soup.find('title')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "    else:\n",
    "        title = \"No <title> tag found\"\n",
    "    \n",
    "    # Close the Selenium driver\n",
    "    driver.quit()\n",
    "    \n",
    "    return title\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate daily returns\n",
    "def calculate_daily_returns(prices):\n",
    "    return prices.pct_change().dropna()\n",
    "\n",
    "# Function to calculate 22-day returns\n",
    "def calculate_22_day_returns(prices):\n",
    "    return prices.pct_change(periods=22).dropna()\n",
    "\n",
    "# Function to calculate statistics on 22-day returns\n",
    "def calculate_22_day_statistics(returns_22):\n",
    "    last_66_returns = returns_22.iloc[-66:]  # Use iloc for integer-based indexing\n",
    "    min_return = last_66_returns.min()\n",
    "    max_return = last_66_returns.max()\n",
    "    first_quartile = np.percentile(last_66_returns, 25)\n",
    "    return min_return, max_return, first_quartile\n",
    "\n",
    "\n",
    "# Main function to process the data\n",
    "def process_data(url, url_title, filename):\n",
    "    # Download the CSV file\n",
    "    download_csv(url, filename)\n",
    "\n",
    "    # Read the CSV into a DataFrame with proper settings\n",
    "    try:\n",
    "        df = pd.read_csv(filename, on_bad_lines='skip', delimiter=',', decimal='.', encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"⚠️ The DataFrame is empty.\")\n",
    "        return None\n",
    "\n",
    "    # Strip any leading or trailing whitespace from the column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # Ensure the date column is in datetime format\n",
    "    date_column = 'Data'  # The actual name of the date column\n",
    "    if date_column not in df.columns:\n",
    "        print(f\"⚠️ Warning: Column '{date_column}' not found. Available columns: {df.columns}\")\n",
    "        return None\n",
    "\n",
    "    # Convert date column\n",
    "    df['date'] = pd.to_datetime(df[date_column])\n",
    "\n",
    "    # Sort the data by date in ascending order\n",
    "    df = df.sort_values(by='date')\n",
    "\n",
    "    # Calculate daily returns from the second column (prices)\n",
    "    prices_column = df.columns[1]  # Assuming the second column contains the prices\n",
    "    daily_returns = calculate_daily_returns(df[prices_column])\n",
    "\n",
    "    # Calculate 22-day returns\n",
    "    returns_22 = calculate_22_day_returns(df[prices_column])\n",
    "\n",
    "    # Check if we have enough data points\n",
    "    if len(daily_returns) == 0 or len(returns_22) == 0:\n",
    "        print(\"⚠️ Not enough data to calculate returns.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate statistics for the last 66 22-day returns\n",
    "    min_return, max_return, first_quartile = calculate_22_day_statistics(returns_22)\n",
    "\n",
    "    # Get the newest 1-day and 22-day returns\n",
    "    newest_1_day_return = daily_returns.iloc[-1] if len(daily_returns) > 0 else None\n",
    "    newest_22_day_return = returns_22.iloc[-1] if len(returns_22) > 0 else None\n",
    "\n",
    "    # Get the title of the webpage\n",
    "    nazwa = get_webpage_title(url_title)\n",
    "    if nazwa == \"Wyszukiwanie symbolu - Stooq\" or nazwa == \"No <title> tag found\":\n",
    "        print(\"⚠️ Brak symbolu w bazie Stooq\")\n",
    "        return None\n",
    "    \n",
    "    # Save the required data into an array\n",
    "    result = [\n",
    "        nazwa,  # Name of the file\n",
    "        min_return,  # Minimum of the last 66 22-day returns\n",
    "        max_return,  # Maximum of the last 66 22-day returns\n",
    "        first_quartile,  # First quartile of the last 66 22-day returns\n",
    "        newest_1_day_return,  # Newest 1-day return\n",
    "        newest_22_day_return  # Newest 22-day return\n",
    "    ]\n",
    "    \n",
    "    print(\"✅ Processed Data:\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fac899-f191-4a8b-8c4b-f386c42344a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each XXXX value\n",
    "for xxxx in range(1000, 1010):\n",
    "    csv_url = csv_base_url.format(xxxx)\n",
    "    title_url = title_base_url.format(xxxx)\n",
    "\n",
    "    print(f\"Processing: {csv_url}\")\n",
    "    result = process_data(csv_url, title_url, csv_filename)\n",
    "\n",
    "    all_results.append(result)\n",
    "   # print(\"✅ Processed and appended Data:\", all_results)\n",
    "    # Delete CSV file after processing\n",
    "    if os.path.exists(csv_filename):\n",
    "        os.remove(csv_filename)\n",
    "    \n",
    "    time.sleep(2)  # Optional delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bee3ebf-0596-4d8f-badb-03447ce8a71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Min 22-day Return  \\\n",
      "0  1006.N - Goldman Sachs Parasol FIO Globalnej D...          -0.040077   \n",
      "1       1007.N - MetLife PPE Fundusz Akcji B - Stooq          -0.016511   \n",
      "2                1008.N - PKO Emerytura 2065 - Stooq          -0.022240   \n",
      "3         1009.N - PKO Obligacji Uniwersalny - Stooq           0.003936   \n",
      "4  1006.N - Goldman Sachs Parasol FIO Globalnej D...          -0.040077   \n",
      "\n",
      "   Max 22-day Return  1st Quartile  Latest 1-day Return  Latest 22-day Return  \n",
      "0           0.042157     -0.014637            -0.006401             -0.040077  \n",
      "1           0.116253      0.019590             0.004369              0.050254  \n",
      "2           0.080422      0.005791             0.006084              0.005319  \n",
      "3           0.007159      0.004869             0.000169              0.005518  \n",
      "4           0.042157     -0.014637            -0.006401             -0.040077  \n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame and save\n",
    "cleaned_results = [row for row in all_results if row is not None]\n",
    "final_df = pd.DataFrame(cleaned_results, columns=[\"Title\", \"Min 22-day Return\", \"Max 22-day Return\", \"1st Quartile\", \"Latest 1-day Return\", \"Latest 22-day Return\"])\n",
    "csv_data = final_df.to_csv(index=False, header=True, sep=\";\").encode('utf-8')\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950e279-3c3e-4697-bfa0-9ce2e1990625",
   "metadata": {},
   "outputs": [],
   "source": [
    "creds = service_account.Credentials.from_service_account_file('/tmp/credentials.json', scopes=['https://www.googleapis.com/auth/drive'])\n",
    "\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# File details\n",
    "file_name = \"oceny.csv\"\n",
    "\n",
    "# Correctly get the folder ID (replace 'Dane' with the actual folder name)\n",
    "folder_name = 'Dane'\n",
    "query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
    "results = drive_service.files().list(q=query, spaces='drive', fields='files(id)').execute()\n",
    "items = results.get('files', [])\n",
    "\n",
    "if items:\n",
    "    folder_id = items[0]['id']\n",
    "else:\n",
    "    print(f\"❌ Folder '{folder_name}' not found in Google Drive.\")\n",
    "    exit()\n",
    "\n",
    "# Now use the correct folder ID in the file search query\n",
    "query = f\"name='{file_name}' and '{folder_id}' in parents\"\n",
    "results = drive_service.files().list(q=query, spaces='drive', fields='files(id)').execute()\n",
    "items = results.get('files', [])\n",
    "\n",
    "if items:\n",
    "    file_id = items[0]['id']\n",
    "    # Update the existing file as a new version\n",
    "    # The MediaIoBaseUpload class needs to be called directly, not as an attribute of drive_service.\n",
    "    media = MediaIoBaseUpload(io.BytesIO(csv_data), mimetype='text/csv', resumable=True)\n",
    "    updated_file = drive_service.files().update(fileId=file_id, media_body=media).execute()\n",
    "    print(f\"File '{file_name}' updated as a new version. File ID: {file_id}\")\n",
    "else:\n",
    "    # If the file doesn't exist, create it\n",
    "    file_metadata = {\n",
    "        'name': file_name,\n",
    "        'parents': [folder_id] \n",
    "    }\n",
    "    # The MediaIoBaseUpload class needs to be called directly, not as an attribute of drive_service.\n",
    "    media = MediaIoBaseUpload(io.BytesIO(csv_data), mimetype='text/csv', resumable=True)\n",
    "    created_file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    file_id = created_file.get('id')\n",
    "    print(f\"File '{file_name}' created. File ID: {file_id}\")\n",
    "\n",
    "print(f\"Analysis complete. Extracted data from {len(all_results)} pages saved in 'oceny.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c1c21-698a-4f49-a053-b14188ef2149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
